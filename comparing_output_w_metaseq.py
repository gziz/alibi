import torch
from alibi_generator import ALiBiAttentionMaskGenerator
from metaseq import buffered_future_mask

torch.set_printoptions(precision=6, sci_mode=False)

def main():
    seq_len = 20
    batch_size = 2 # arbitrary, only affects metaseq_mask shape
    model_size = 256 # arbitrary, no effect in output
    num_heads = 8

    alibi_gen = ALiBiAttentionMaskGenerator(num_heads=num_heads)
    fairseq2_mask = alibi_gen(torch.empty(seq_len, model_size))

    #Â num_heads is hard coded to 8 inside buffered_future_mask
    metaseq_mask = buffered_future_mask(torch.empty((seq_len, batch_size)))
    
    # Regardless of batch_size, for metaseq_mask, we only take into consideration 
    # the first batch. i.e. the first num_heads tensors in metaseq_mask
    # For fairseq2, the batch_size extension is generated by the decoder.
    print(torch.equal(fairseq2_mask, metaseq_mask[:num_heads]))

if __name__ == '__main__':
    main()